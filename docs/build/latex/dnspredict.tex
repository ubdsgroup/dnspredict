%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{dnspredict Documentation}
\date{Jan 18, 2021}
\release{2020}
\author{Varun Chandola}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Manual}
\label{\detokenize{manual:manual}}\label{\detokenize{manual::doc}}
\sphinxcode{\sphinxupquote{dnspredict}} is a Python implementation of a \sphinxstyleemphasis{Long Short\sphinxhyphen{}Term Memory} (LSTM) network for the predictive modeling of the Domain Name Server (DNS) system. The model is trained to predict the volume of DNS queries that the system will encounter in the next time\sphinxhyphen{}interval for a given domain. A statistical anomaly detection module monitors the difference between the predicted and observed volumes to identify system\sphinxhyphen{}wide anomalies in the query counts. The following figure shows the overview of the predictive model, coupled with the anomaly detection module.

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=600\sphinxpxdimen]{{overview}.png}\hspace*{\fill}}

The input to the model is a time\sphinxhyphen{}series of vector of location\sphinxhyphen{}wise counts for a given domain. Optionally, \sphinxcode{\sphinxupquote{nxdomain}} query counts can also be included in the input vector. The neural network consists of one or more layers of LSTM nodes, that combine the inputs in a non\sphinxhyphen{}linear fashion, along with the historical signal (or \sphinxstyleemphasis{memory}). The number of LSTM nodes in a layer correspond to the short\sphinxhyphen{}term memory used by the layer.

An optional \sphinxstylestrong{attention} layer can be included after the first LSTM layer. This layer allows the model to pay selective attention (or focus) to a few inputs, and can improve the performance. Additionally, the output of the attention layer can also allow us to explain the model outputs, in terms of the inputs. This will be illustrated in the experimental results.


\section{Requirements}
\label{\detokenize{manual:requirements}}
The implementation has been tested on \sphinxcode{\sphinxupquote{python 3.7.3}} and requires following packages:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
Package
&\sphinxstyletheadfamily 
Version
\\
\hline
\sphinxcode{\sphinxupquote{scikit\sphinxhyphen{}learn}}
&
0.21.2
\\
\hline
\sphinxcode{\sphinxupquote{numpy}}
&
1.16.4
\\
\hline
\sphinxcode{\sphinxupquote{pandas}}
&
0.24.2
\\
\hline
\sphinxcode{\sphinxupquote{tensorflow}}
&
1.13.1
\\
\hline
\sphinxcode{\sphinxupquote{keras}}
&
2.3.1
\\
\hline
\sphinxcode{\sphinxupquote{plotly}}
&
4.14.1
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\begin{sphinxadmonition}{note}{Note:}
The \sphinxcode{\sphinxupquote{plotly}} library is only for plotting. While \sphinxcode{\sphinxupquote{keras}} was built using the \sphinxcode{\sphinxupquote{tensorflow}} backend, other backends could work as well.
\end{sphinxadmonition}


\section{Usage}
\label{\detokenize{manual:usage}}
All functions are implemented in the \sphinxcode{\sphinxupquote{dnspredict}} module. Refer to the \sphinxcode{\sphinxupquote{DriverNotebook.ipynb}} notebook for examples.


\subsection{Preparing data}
\label{\detokenize{manual:preparing-data}}
We can either start with the \sphinxtitleref{response.json} file or the \sphinxtitleref{.dat} file and convert it into a format that can be used in the pipeline. The desired \sphinxtitleref{Pandas DataFrame} has the following format:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Index
&\sphinxstyletheadfamily 
Location 1
&\sphinxstyletheadfamily 
Location 2
&\sphinxstyletheadfamily 
…
&\sphinxstyletheadfamily 
Location n
\\
\hline
Time stamp 1
&
count 1
&
count 2
&
…
&
count n
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Two loading functions are provided that can load data from a \sphinxcode{\sphinxupquote{json}} or a \sphinxcode{\sphinxupquote{csv}} file into the above format.
\index{prepData() (in module dnspredict)@\spxentry{prepData()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.prepData}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{prepData}}}{\emph{\DUrole{n}{filename}}, \emph{\DUrole{n}{locs}\DUrole{o}{=}\DUrole{default_value}{{[}{]}}}, \emph{\DUrole{n}{filters}\DUrole{o}{=}\DUrole{default_value}{\{\}}}, \emph{\DUrole{n}{index}\DUrole{o}{=}\DUrole{default_value}{{[}{]}}}, \emph{\DUrole{n}{return\_locs}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Prepare data into a aggregate count time series data frame from a csv file with following format for each line:,count,ip\_version,location,protocol,record\_name,record\_type,time,domain
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{filename}} \textendash{} location of the input file in csv format

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{locs}} \textendash{} list of all locations to be included in the output, missing locations are included with 0 counts. If None, then the locations in the given file are used (default \sphinxhyphen{} {[}{]})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{filters}} \textendash{} dictionary containing filters for each column, each filter rule is of the form: ‘column\_name’: list of allowed values (default \sphinxhyphen{} \{\})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{index}} \textendash{} external time index provided to reindex the data frame (default \sphinxhyphen{} {[}{]})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{return\_locs}} \textendash{} boolean indicator, if True, then the locs parameter is returned (default \sphinxhyphen{} False)

\end{itemize}

\item[{Returns}] \leavevmode
dataframe containing aggregated counts by location

\end{description}\end{quote}

\end{fulllineitems}

\index{prepDataJSON() (in module dnspredict)@\spxentry{prepDataJSON()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.prepDataJSON}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{prepDataJSON}}}{\emph{\DUrole{n}{filename}}, \emph{\DUrole{n}{locs}\DUrole{o}{=}\DUrole{default_value}{{[}{]}}}, \emph{\DUrole{n}{filters}\DUrole{o}{=}\DUrole{default_value}{\{\}}}, \emph{\DUrole{n}{index}\DUrole{o}{=}\DUrole{default_value}{{[}{]}}}, \emph{\DUrole{n}{return\_locs}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Prepare data into a aggregate count time series data frame from the json file containing query records.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{filename}} \textendash{} location of the input file in json format

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{locs}} \textendash{} list of all locations to be included in the output, missing locations are included with 0 counts. If None, then the locations in the given file are used (default \sphinxhyphen{} {[}{]})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{filters}} \textendash{} dictionary containing filters for each column, each filter rule is of the form: ‘column\_name’: list of allowed values (default \sphinxhyphen{} \{\})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{index}} \textendash{} external time index provided to reindex the data frame (default \sphinxhyphen{} {[}{]})

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{return\_locs}} \textendash{} boolean indicator, if True, then the locs parameter is returned (default \sphinxhyphen{} False)

\end{itemize}

\item[{Returns}] \leavevmode
dataframe containing aggregated counts by location

\end{description}\end{quote}

\end{fulllineitems}


The \sphinxcode{\sphinxupquote{filter}} parameter can be used to create desired subsets for analysis. For instance, if one is interested in analyzing data for a single location (e.g., \sphinxtitleref{1}), for only \sphinxtitleref{udp} and \sphinxtitleref{tcp} protocols and for a single domain (e.g., \sphinxtitleref{dnsmadeeasy.com}), one can prepare data as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{dnspredict} \PYG{k+kn}{import} \PYG{o}{*}

\PYG{n}{filters} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{protocol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{udp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tcp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{domain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dnsmadeeasy.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{prepData}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,}\PYG{n}{locs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{filters}\PYG{o}{=}\PYG{n}{filters}\PYG{p}{)}
\end{sphinxVerbatim}

Similarly, one can pick out the \sphinxcode{\sphinxupquote{nxdomain}} counts using the same function, but with an additional condition in the filter:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{filters\PYGZus{}nx}\PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{protocol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{udp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tcp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{record\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nxdomain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{domain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dnsmadeeasy.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{data\PYGZus{}nx} \PYG{o}{=} \PYG{n}{prepData}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,}\PYG{n}{locs}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{filters}\PYG{o}{=}\PYG{n}{filters}\PYG{p}{,}\PYG{n}{index}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}
\end{sphinxVerbatim}

Note that you can pass an explicit \sphinxtitleref{index} so that the index of the returned data set matches the index from the previous data set.

The data can be visualized using the \sphinxcode{\sphinxupquote{plotly}} library:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{px}\PYG{o}{.}\PYG{n}{line}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,}\PYG{n}{x}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{,}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dnsmadeeasy.com}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=600\sphinxpxdimen]{{sampledata}.png}\hspace*{\fill}}


\subsection{Preparing training data}
\label{\detokenize{manual:preparing-training-data}}
To train the model, use the following function to prepare the training and test data sets.
\index{prepTFData() (in module dnspredict)@\spxentry{prepTFData()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.prepTFData}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{prepTFData}}}{\emph{\DUrole{n}{data}}, \emph{\DUrole{n}{locs}}, \emph{\DUrole{n}{include\_nx}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{data\_nx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{history}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{num\_timepoints}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{scaler}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Prepare the data in the tensorflow format.
\begin{quote}\begin{description}
\item[{Input data}] \leavevmode
dataframe containing counts data for a given domain

\item[{Input locs}] \leavevmode
list of target locations

\item[{Input include\_nx}] \leavevmode
flag to indicate if the nxdomain counts have to be included (default False)

\item[{Input data\_nx}] \leavevmode
optional dataframe containing nxdomain counts (required if \sphinxcode{\sphinxupquote{include\_nx = True}})

\item[{Input history}] \leavevmode
length of history for the predictive model (default 1)

\item[{Input num\_timepoints}] \leavevmode
length of time series in each batch (default 1)

\item[{Scaler}] \leavevmode
optional scaler tuple to scale the data (for creating test data)

\item[{Return X}] \leavevmode
training input \sphinxhyphen{} A (n1 x n2 x n3) \sphinxcode{\sphinxupquote{numpy}} array. n1 is equal to \sphinxcode{\sphinxupquote{ceil(data.shape{[}0{]}/num\_timepoints)}}. X is padded if the number of rows in data is not an exact multiple of num\_timepoints. n2 is equal to num\_timepoints. n3 is equal to (history x length of locs) if \sphinxcode{\sphinxupquote{include\_nx==False}}, otherwise (2 x history x length of locs).

\item[{Return Y}] \leavevmode
training targets \sphinxhyphen{} A (n1 x n2 x n3) \sphinxcode{\sphinxupquote{numpy}} array. n1 is equal to \sphinxcode{\sphinxupquote{ceil(data.shape{[}0{]}/num\_timepoints)}}. Y is padded if the number of rows in data is not an exact multiple of num\_timepoints. n2 is equal to num\_timepoints. n3 is equal to (history x length of locs).

\item[{Return scaler}] \leavevmode
minmax scaler tuple used to normalize data (only if scaler == None)

\end{description}\end{quote}

\end{fulllineitems}


The \sphinxtitleref{prepTFData} function can be used to create the training and test data sets for analysis. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{target\PYGZus{}locs} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}
\PYG{n}{include\PYGZus{}nx} \PYG{o}{=} \PYG{k+kc}{False}

\PYG{n}{train\PYGZus{}start} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2019\PYGZhy{}07\PYGZhy{}24 00:00:00}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{train\PYGZus{}end} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2019\PYGZhy{}08\PYGZhy{}21 23:59:59}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{history} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of consecutive observations to concatenate as a \PYGZdq{}single observation\PYGZdq{}}
\PYG{n}{num\PYGZus{}timepoints} \PYG{o}{=} \PYG{l+m+mi}{144}

\PYG{n}{df\PYGZus{}train} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{train\PYGZus{}start}\PYG{p}{:}\PYG{n}{train\PYGZus{}end}\PYG{p}{]}
\PYG{n}{df\PYGZus{}train\PYGZus{}nx} \PYG{o}{=} \PYG{n}{data\PYGZus{}nx}\PYG{p}{[}\PYG{n}{train\PYGZus{}start}\PYG{p}{:}\PYG{n}{train\PYGZus{}end}\PYG{p}{]}
\PYG{n}{trainX\PYGZus{}arr}\PYG{p}{,}\PYG{n}{trainY\PYGZus{}arr}\PYG{p}{,}\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{prepTFData}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{p}{,}
                                  \PYG{n}{target\PYGZus{}locs}\PYG{p}{,}\PYG{n}{history}\PYG{o}{=}\PYG{n}{history}\PYG{p}{,}
                                  \PYG{n}{num\PYGZus{}timepoints}\PYG{o}{=}\PYG{n}{num\PYGZus{}timepoints}\PYG{p}{,}\PYG{n}{include\PYGZus{}nx} \PYG{o}{=} \PYG{n}{include\PYGZus{}nx}\PYG{p}{,}
                                  \PYG{n}{data\PYGZus{}nx}\PYG{o}{=}\PYG{n}{df\PYGZus{}train\PYGZus{}nx}\PYG{p}{)}
\end{sphinxVerbatim}

The above code will create the training data set and the scaler.

The \sphinxcode{\sphinxupquote{history}} and \sphinxcode{\sphinxupquote{num\_timepoints}} parameters have a similar impact on the resulting model. Both can be used to specify the size of the short term history in the model. While \sphinxcode{\sphinxupquote{history}} concatenates several consecutive observations to create one input vector, and thus explicitly models the short\sphinxhyphen{}term dependencies, \sphinxcode{\sphinxupquote{num\_timepoints}} uses the original input vector, but \sphinxcode{\sphinxupquote{num\_timepoints}} observations are presented in a single batch and the model maintains the history over \sphinxcode{\sphinxupquote{num\_timepoints}} consecutive LSTM units. The number of parameters to be trained are fewer in the latter case.


\subsection{Building the model}
\label{\detokenize{manual:building-the-model}}
The LSTM model is created using the following function.
\index{buildLSTMModel() (in module dnspredict)@\spxentry{buildLSTMModel()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.buildLSTMModel}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{buildLSTMModel}}}{\emph{\DUrole{n}{input\_shape}}, \emph{\DUrole{n}{output\_units}}, \emph{\DUrole{n}{n\_units}\DUrole{o}{=}\DUrole{default_value}{8}}, \emph{\DUrole{n}{n\_layers}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{l1}\DUrole{o}{=}\DUrole{default_value}{0.01}}, \emph{\DUrole{n}{l2}\DUrole{o}{=}\DUrole{default_value}{0.01}}, \emph{\DUrole{n}{attention}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Build the LSTM model for multivariate time series prediction
\begin{quote}\begin{description}
\item[{Input input\_shape}] \leavevmode
\sphinxcode{\sphinxupquote{tuple}} (length of history,number of input features)

\item[{Input output\_units}] \leavevmode
number of targets in the multivariate output

\item[{Input n\_units}] \leavevmode
Dimensionality of the hidden vector for LSTM (default 8)

\item[{Input n\_layers}] \leavevmode
Number of LSTM layers in the model (default 1)

\item[{Input l1}] \leavevmode
l1\sphinxhyphen{}regularization penalty (default 0.01)

\item[{Input l2}] \leavevmode
l2\sphinxhyphen{}regularization penalty (default 0.01)

\item[{Input attention}] \leavevmode
boolean flag to indicate inclusion of an attention layer (default False)

\item[{Returns}] \leavevmode
\sphinxcode{\sphinxupquote{keras}} LSTM model

\end{description}\end{quote}

\end{fulllineitems}


The \sphinxcode{\sphinxupquote{buildLSTMModel}} permits increasing the complexity of the model (for modeling complex relationship between the input and output) by adding more LSTM layers (\sphinxtitleref{n\_layers}) and increasing the size of the output of each LSTM layer (\sphinxtitleref{n\_units}). The overfitting can be controlled by increasing the \sphinxcode{\sphinxupquote{l1}} and \sphinxcode{\sphinxupquote{l2}} regularization parameters. Additionally, setting the \sphinxcode{\sphinxupquote{attention}} flag to \sphinxcode{\sphinxupquote{True}} adds an \sphinxstyleemphasis{attention} layer immediately after the first LSTM layer to focus the attention and allow for explainability.

A model can be created as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{attention} \PYG{o}{=} \PYG{k+kc}{False}
\PYG{n}{n\PYGZus{}layers} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{n\PYGZus{}units} \PYG{o}{=} \PYG{l+m+mi}{8}
\PYG{n}{l1} \PYG{o}{=} \PYG{l+m+mf}{0.01}
\PYG{n}{l2} \PYG{o}{=} \PYG{l+m+mf}{0.01}

\PYG{n}{input\PYGZus{}shape} \PYG{o}{=} \PYG{p}{(}\PYG{n}{trainX\PYGZus{}arr}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{trainX\PYGZus{}arr}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{output\PYGZus{}units} \PYG{o}{=} \PYG{n}{trainY\PYGZus{}arr}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{n}{trainY\PYGZus{}arr}\PYG{o}{.}\PYG{n}{ndim}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{buildLSTMModel}\PYG{p}{(}\PYG{n}{input\PYGZus{}shape}\PYG{p}{,}\PYG{n}{output\PYGZus{}units}\PYG{p}{,}\PYG{n}{n\PYGZus{}units}\PYG{p}{,}\PYG{n}{n\PYGZus{}layers}\PYG{p}{,}\PYG{n}{l1}\PYG{p}{,}\PYG{n}{l2}\PYG{p}{,}\PYG{n}{attention}\PYG{o}{=}\PYG{n}{attention}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Training the model}
\label{\detokenize{manual:training-the-model}}
To train the model, execute the following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}epochs} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{verbose} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{}this controls the level of log messages during training}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{trainX\PYGZus{}arr}\PYG{p}{,}\PYG{n}{trainY\PYGZus{}arr}\PYG{p}{,}\PYG{n}{epochs}\PYG{o}{=}\PYG{n}{n\PYGZus{}epochs}\PYG{p}{,}\PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{verbose}\PYG{o}{=}\PYG{n}{verbose}\PYG{p}{,}\PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

The trained model can be saved to the disk and loaded back using the following two commands:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model}\PYG{o}{.}\PYG{n}{save}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/path/to/location}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{load\PYGZus{}model}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path/to/location}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Using the model for predictions}
\label{\detokenize{manual:using-the-model-for-predictions}}
To prepare test data, pass the scalers from above to scale the test data appropriately:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{test\PYGZus{}start} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2019\PYGZhy{}08\PYGZhy{}22 00:00:00}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{test\PYGZus{}end} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2019\PYGZhy{}08\PYGZhy{}31 23:59:59}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{df\PYGZus{}test} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{test\PYGZus{}start}\PYG{p}{:}\PYG{n}{test\PYGZus{}end}\PYG{p}{]}
\PYG{n}{df\PYGZus{}test\PYGZus{}nx} \PYG{o}{=} \PYG{n}{data\PYGZus{}nx}\PYG{p}{[}\PYG{n}{test\PYGZus{}start}\PYG{p}{:}\PYG{n}{test\PYGZus{}end}\PYG{p}{]}
\PYG{n}{testX\PYGZus{}arr}\PYG{p}{,}\PYG{n}{testY\PYGZus{}arr} \PYG{o}{=} \PYG{n}{prepTFData}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{,}
                       \PYG{n}{target\PYGZus{}locs}\PYG{p}{,}\PYG{n}{history}\PYG{o}{=}\PYG{n}{history}\PYG{p}{,}
                       \PYG{n}{num\PYGZus{}timepoints}\PYG{o}{=}\PYG{n}{num\PYGZus{}timepoints}\PYG{p}{,}
                       \PYG{n}{include\PYGZus{}nx} \PYG{o}{=} \PYG{n}{include\PYGZus{}nx}\PYG{p}{,}\PYG{n}{data\PYGZus{}nx}\PYG{o}{=}\PYG{n}{df\PYGZus{}test\PYGZus{}nx}\PYG{p}{,}\PYG{n}{scaler}\PYG{o}{=}\PYG{n}{scaler}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{warning}{Warning:}
Make sure that the test data is scaled in the same manner as the training data.
\end{sphinxadmonition}

To test the model on a new data set, the function \sphinxcode{\sphinxupquote{modelPredict}} is used:
.. currentmodule:: dnspredict
.. autofunction:: modelPredict

To generate predictions, execute the following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}preds} \PYG{o}{=} \PYG{n}{modelPredict}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}\PYG{n}{testX\PYGZus{}arr}\PYG{p}{,}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{,}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,}\PYG{n}{scaler}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

To visually compare the observed and predicted counts for a given location:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{df\PYGZus{}loc\PYGZus{}single} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{df\PYGZus{}test}\PYG{p}{[}\PYG{n}{loc}\PYG{p}{]}\PYG{p}{,}\PYG{n}{df\PYGZus{}preds}\PYG{p}{[}\PYG{n}{loc}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{px}\PYG{o}{.}\PYG{n}{line}\PYG{p}{(}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{,}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Domain }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{ Location }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dnsmadeeasy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{loc}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Anomaly detection}
\label{\detokenize{manual:anomaly-detection}}
For anomaly detection, first the parameters are estimated by obtaining predictions on the training data set using the trained model and the estimating mean and standard deviations for the residuals, using the \sphinxcode{\sphinxupquote{estimateADParams}} function:
\index{estimateADParams() (in module dnspredict)@\spxentry{estimateADParams()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.estimateADParams}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{estimateADParams}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{trainX\_arr}}, \emph{\DUrole{n}{trainY\_arr}}, \emph{\DUrole{n}{train\_shape}}, \emph{\DUrole{n}{target\_scaler}}}{}
Estimate the mean and standard deviation parameters obtained using the model on 
the training data.
\begin{quote}\begin{description}
\item[{Input model}] \leavevmode
\sphinxcode{\sphinxupquote{keras}} model for predicting query counts

\item[{Input trainX\_arr}] \leavevmode
Input \sphinxtitleref{numpy} array used for training

\item[{Input trainY\_arr}] \leavevmode
Target \sphinxtitleref{numpy} array used for training

\item[{Input train\_shape}] \leavevmode
Shape (tuple) of the original training data

\item[{Input target\_scaler}] \leavevmode
Target scaler to rescale the predictions

\item[{Returns}] \leavevmode
tuple containing \sphinxtitleref{pandas DataFrame} for location\sphinxhyphen{}wise residual means and standard deviation

\end{description}\end{quote}

\end{fulllineitems}


The parameters are estimated as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ad\PYGZus{}params} \PYG{o}{=} \PYG{n}{estimateADParams}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}\PYG{n}{trainX\PYGZus{}arr}\PYG{p}{,}\PYG{n}{trainY\PYGZus{}arr}\PYG{p}{,}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,}\PYG{n}{scaler}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

To generate anomaly labels for test data, use the \sphinxcode{\sphinxupquote{detectAnomalies}} function:
\index{detectAnomalies() (in module dnspredict)@\spxentry{detectAnomalies()}\spxextra{in module dnspredict}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{manual:dnspredict.detectAnomalies}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{dnspredict.}}\sphinxbfcode{\sphinxupquote{detectAnomalies}}}{\emph{\DUrole{n}{df\_test}}, \emph{\DUrole{n}{df\_preds}}, \emph{\DUrole{n}{res\_stdev}}, \emph{\DUrole{n}{thresh}\DUrole{o}{=}\DUrole{default_value}{3}}}{}
Assigns an anomaly label using the model predictions and the observed data.
\begin{quote}\begin{description}
\item[{Input df\_test}] \leavevmode
\sphinxcode{\sphinxupquote{pandas DataFrame}} containing the observed data

\item[{Input df\_preds}] \leavevmode
\sphinxcode{\sphinxupquote{pandas DataFrame}} containing the model predictions

\item[{Input res\_stdev}] \leavevmode
standard deviations for each location

\item[{Input thresh}] \leavevmode
integer threshold to determine anomalies (default 3)

\item[{Returns}] \leavevmode
DataFrame containing anomaly labels

\end{description}\end{quote}

\end{fulllineitems}


Using the model predictions obtained above, the anomaly scores can be obtained as:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}anomalies} \PYG{o}{=} \PYG{n}{detectAnomalies}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{,}\PYG{n}{df\PYGZus{}preds}\PYG{p}{,}\PYG{n}{ad\PYGZus{}params}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{thresh}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

The following code can be used to plot the observed and predicted counts and the corresponding anomaly labels:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{plotly}\PYG{n+nn}{.}\PYG{n+nn}{subplots} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}subplots}
\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{df\PYGZus{}loc\PYGZus{}single} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{df\PYGZus{}test}\PYG{p}{[}\PYG{n}{loc}\PYG{p}{]}\PYG{p}{,}\PYG{n}{df\PYGZus{}preds}\PYG{p}{[}\PYG{n}{loc}\PYG{p}{]}\PYG{p}{,}\PYG{n}{df\PYGZus{}anomalies}\PYG{p}{[}\PYG{n}{loc}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Anomaly Label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{subfig} \PYG{o}{=} \PYG{n}{make\PYGZus{}subplots}\PYG{p}{(}\PYG{n}{specs}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{secondary\PYGZus{}y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{\PYGZcb{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{px}\PYG{o}{.}\PYG{n}{line}\PYG{p}{(}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Domain }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{ Location }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dnsmadeeasy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{loc}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{fig2} \PYG{o}{=} \PYG{n}{px}\PYG{o}{.}\PYG{n}{line}\PYG{p}{(}\PYG{n}{df\PYGZus{}loc\PYGZus{}single}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Anomaly Label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{webgl}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{p}{)}
\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{update\PYGZus{}traces}\PYG{p}{(}\PYG{n}{yaxis}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{add\PYGZus{}traces}\PYG{p}{(}\PYG{n}{fig}\PYG{o}{.}\PYG{n}{data} \PYG{o}{+} \PYG{n}{fig2}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Time}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{yaxis}\PYG{o}{.}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Query Counts}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{layout}\PYG{o}{.}\PYG{n}{yaxis2}\PYG{o}{.}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Anomaly Label}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{for\PYGZus{}each\PYGZus{}trace}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{t}\PYG{p}{:} \PYG{n}{t}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{line}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{n}{t}\PYG{o}{.}\PYG{n}{marker}\PYG{o}{.}\PYG{n}{color}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{subfig}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent{\hspace*{\fill}\sphinxincludegraphics[width=800\sphinxpxdimen]{{sampleanomalies}.png}\hspace*{\fill}}



\renewcommand{\indexname}{Index}
\printindex
\end{document}